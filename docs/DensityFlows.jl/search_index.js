var documenterSearchIndex = {"docs":
[{"location":"#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"DensityFlows.jl is a lightweight Julia package for data scientists and physicists who want a simple way to model low-dimensional probability distributions using normalizing flows. It’s built for clarity and ease of use — ideal for anyone who wants to experiment, learn, or prototype quickly without the overhead of large ML frameworks. While other libraries focus on complex, high-dimensional tasks like image generation, DensityFlows.jl keeps things minimal and transparent, helping you understand and apply normalizing flows right away.","category":"page"},{"location":"#Basics-of-normalizing-flows","page":"Overview","title":"Basics of normalizing flows","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Let us say we want to emulate a conditional probability P with distribution function p(x   theta) (equivalent to a likelihood) for xin mathcalD subset mathbbR^d theta in mathcalE subset mathbbR^n, with d in mathbbN_* and n in mathbbN. To that end we can start from a probability distribution function Q with distribution function q that is known and perform a change of variable from q to p. In practice, we thus want to find the diffeomorphism f_theta that, for z sim Q satisfies f_theta(z) sim P. This requirement imposes that f_theta satisfies","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"p(x   theta) = q(f_theta^-1(x)) left rm det   Jf_theta^-1(x) right quad forall (x theta) in mathcalD times mathcalE","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"with Jf_theta^-1 the Jacobian of the inverse transformation. Moreover, using the properties of the Jacobian, this can also be written","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"p(x   theta) = fracq(f_theta^-1(x))left rm det   Jf_theta(f^-1_theta(x)) right quad forall (x theta) in mathcalD times mathcalE","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Now, let us assume that f_theta is written as a composition of m elementary diffeomorphisms as follows","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"f_theta = g_theta m circ g_theta m-1 circ dots circ g_theta 1","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"These diffeomorphisms can be defined using neural networks. Then, using the chain rule, for theta in mathcalE and z in f_theta^-1(mathcalD), ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\nbeginsplit\nJf_theta(z)  = Jg_theta m circ g_theta m-1 circ dots circ  g_theta 3circ g_theta 2 circ g_theta 1(z) \n  = Jg_theta m circ g_theta m-1 circ dots circ g_theta 3 circ g_theta 2(g_theta 1 (z)) times  Jg_theta 1(z) \n   = Jg_theta m circ g_theta m-1 circ dots circ g_theta 3(g_theta 2 circ g_theta 1(z)) times  Jg_theta 2(g_theta 1 (z)) times  Jg_theta 1(z) \n  = dots \n  = prod_i=1^m Jg_theta i(g_theta i-1 circ dots circ g_theta 1(z))  \nendsplit\nendequation*","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Let us now apply this relationship to z = f_theta^-1(x) where xin mathcalD,","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\nbeginsplit\nJf_theta(f^-1_theta(x))  = prod_i=1^m Jg_theta i(g_theta i-1 circ dots circ g_theta 1 circ g_theta 1^-1  circ dots circ g_theta m^-1 (x))\n = prod_i=1^m Jg_theta i(g_theta i^-1  circ dots circ g_theta m^-1 (x))  \nendsplit\nendequation*","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"and therefore","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"rm det   Jf_theta(f^-1_theta(x))  = prod_i=1^m rm det  Jg_theta i(g_theta i^-1  circ dots circ g_theta m^-1 (x)) ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"In other words, we have shown that the determinant of the Jacobian can be computed recursively by multiplying the Jacobian of every diffeomorphism evaluated at a point that only depends on the previous inverse diffeomorphisms. In practice for a large number of dimension the jacobian can be long to evaluate. One solution is to use transformation with triangular jacobians which can be computed much faster.","category":"page"},{"location":"#Loss-function","page":"Overview","title":"Loss function","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The loss function associated to the determination of f_theta is the Kullback-Leibler divergence between p and the sampled distribution r","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\nbeginsplit\nL  = int  r(x    theta) lnfracr(x    theta)p(x    theta)  rm d x \n = - mathbbE_r leftln p(x    theta)    theta right + rm cst \n = - mathbbE_r leftln q(f_theta^-1(x)) - ln left rm det   Jf_theta(f^-1_theta(x)) right    thetaright \nendsplit\nendequation*","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"For a sample of N points (x theta)_i_iin 1 N it can be estimated as","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\nbeginsplit\nL  simeq frac1N sum_i = 1^N leftln q(f_theta_i^-1(x_i)) - sum_j=1^m ln left rm det   Jg_theta_i j(g_theta_i j^-1  circ dots circ g_theta_i m^-1 (x))  right right  \nendsplit\nendequation*","category":"page"},{"location":"#Layers","page":"Overview","title":"Layers","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Currently, only NICE and RNVP coupling layers are implemented. They are, triangular layers, which make the evaluation of their jacobian much easier. In practice, every layer can be described as a function mapping a d dimensional vector to another d dimensional vector, which indices are computed as follows","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\ng_theta(z)_ell = \nbegincases\nz_ell e^s_elllefttheta (z_k)_k in mathcalPright + t_ell lefttheta (z_k)_k in mathcalPright quad rm if quad ell  in overlinemathcalP \nz_ell quad rm if quad ell in mathcalP \nendcases\nqquad forall ell in 1 d  \nendequation*","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"where mathcalP is a non void subset of 1 d. More particularly, let us assume rm card(mathcalP) = a with 0  a  d. Then, the layers transform the dimensions that are in overlinemathcalP and leave the dimensions in mathcalP invariant. In addition, s  mathcalE times mathbbR^a to mathbbR^d-a and t  mathcalE times mathbbR^a to mathbbR^d-a are two arbitrary functions that can be defined using neural networks. In the case of NICE, s=0.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The inverse of this transformation is thus simply","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\ng^-1_theta(x)_ell = \nbegincases\nleft(x_ell - t_ell lefttheta (x_k)_k in mathcalPright right) e^-s_elllefttheta (x_k)_k in mathcalPright  quad rm if quad ell in overlinemathcalP \nx_ell quad rm if quad ell in mathcalP \nendcases\nqquad forall ell in 1 d  \nendequation*","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"and the log jacobian takes the form","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"beginequation*\nln  rm det g^-1_theta(x) = -sum_ell in overlinemathcalP s_elllefttheta (x_k)_k in mathcalPright  \nendequation*","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"For more information see","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Laurent Dinh, David Krueger, Yoshua Bengio (2014). NICE: Non-linear Independent Components Estimation.\nL. Dinh, J. Sohl-Dickstein, S. Bengio (2017). Density Estimation Using Real NVP. International Conference on Learning Representations..","category":"page"},{"location":"#From-here-and-beyond","page":"Overview","title":"From here and beyond","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"In order to get familiar with the code please give a look at the documentation and at the detailed example. Any comment to the code or suggestion for improvement is welcome, please do so using the GitHub issues page if relevant. Further developement should include the implementation of conditional masked autoregressive flows (CMAF).","category":"page"},{"location":"api_coupling/#Couplings","page":"Couplings","title":"Couplings","text":"","category":"section"},{"location":"api_coupling/#Axes","page":"Couplings","title":"Axes","text":"","category":"section"},{"location":"api_coupling/","page":"Couplings","title":"Couplings","text":"Axes define and manipulate dimensions on which the CouplingLayer operates. Some dimensions are left unchanged while the others undergo an affine transformation.","category":"page"},{"location":"api_coupling/#DensityFlows.CouplingAxes","page":"Couplings","title":"DensityFlows.CouplingAxes","text":"CouplingAxes(d, mask; kws )\nCouplingAxes(d, j=d÷2; kws...)\nCouplingAxes(data, mask)\nCouplingAxes(data, j=d÷2; reverse)\n\nCreate axes for CouplingLayer.\n\nArguments\n\nd::Int: dimension of the flow.\nj::Int: dimension cut (default is d÷2).\nmask::AbstractVector{Int}: dimensions that are affected by the coupling.\nn::Int: number of conditions / parameters (default is 0).\nreverse::Bool: (default is false).\n\nThe dimension cut j specifies which dimensions are not modified by the layer.  If reverse is false the layer acts as the identity on dimensions (1, j). If reverse is true  the layer acts as the identity on dimensions (j+1, d).\n\n\n\n\n\n","category":"type"},{"location":"api_coupling/","page":"Couplings","title":"Couplings","text":"Axes can be manipulated with the following functions.","category":"page"},{"location":"api_coupling/#Base.reverse","page":"Couplings","title":"Base.reverse","text":"Base.reverse(axes)\n\nSwap the dimensions that are left unchanged by the layer. See also CouplingAxes.\n\n\n\n\n\n","category":"function"},{"location":"api_coupling/#Coupling-elements","page":"Couplings","title":"Coupling elements","text":"","category":"section"},{"location":"api_coupling/#DensityFlows.CouplingLayer","page":"Couplings","title":"DensityFlows.CouplingLayer","text":"CouplingLayer([T=RNVPCouplingLayer, ] axes; kws...)\nCouplingLayer([T=RNVPCouplingLayer, ] d, j = d ÷ 2; n=0, reverse=false, kws...)\nCouplingLayer([T=RNVPCouplingLayer, ] d, mask; n=0, kws...)\nCouplingLayer([T=RNVPCouplingLayer, ] data, mask; kws...)\nCouplingLayer([T=RNVPCouplingLayer, ] data, j = d ÷ 2; reverse=false, kws...)\n\nCouplingLayer(t_net, axes)\nCouplingLayer(s_net, t_net, axes)\n\nCreate an CouplingLayer with NN models s and t.\n\nThe layer can be represented as a function f such that on dimensions where it does not act  like the identity it returns\n\n    f(x) = x * exp(s) + t quad rm ifforward\n\nand\n\n    f^-1(z) = exp(-s) * (z-t) quad rm ifbackward  \n\nBy default s and t are built with Dense neural networks.\n\nArguments\n\naxes::CouplingAxes.\nd::Int: dimension of the flow.\nj::Int: dimension cut (default is d÷2).\nmask::Vector{Int}: dimensions that are affected by the coupling.\ndata::DataArrays: input data arrays\n\nKeywords arguments\n\nn::Int: number of conditions / parameters (default is 0).\nhidden_dim::Int: number of hidden dimensions in s and t (default is 32).\nn_sublayers_t::Int: number of sublayers in t (default is 2).\nn_sublayers_s::Int: number of sublayers in s (default is 2).\nσ::Function: activation function (default is Flux.relu).\nbias::Bool: activate bias (default is true).\n\nExample\n\njulia> @summary CouplingLayer(3, [1, 3], n=2, hidden_dim=10, n_sublayers_s=1, σ=Flux.tanh)\nRNVPCouplingLayer | s_net > [3, 10, 2] (62 parameters)\n                  | t_net > [3, 10, 10, 2] (172 parameters)\n                  | axes  > (d,n)=(3,2); identity=(2), transformed=(1,3)\n\nSee also CouplingAxes.\n\n\n\n\n\n","category":"type"},{"location":"api_coupling/#DensityFlows.CouplingBlock","page":"Couplings","title":"DensityFlows.CouplingBlock","text":"CouplingBlock(layer_1, layer_2)\nCouplingBlock([T=RNVPCouplingLayer, ] first_axes; kws...)\nCouplingBlock([T=RNVPCouplingLayer, ] d, j = d ÷ 2; n=0, reverse=false, kws...)\nCouplingBlock([T=RNVPCouplingLayer, ] d, mask; n=0, kws...)\n\nCreate an block of two CouplingLayer with opposite / complementary axes.\n\nArguments\n\nfirst_axes::CouplingAxes: axes of the first layer.\nd::Int: dimension of the flow.\nj::Int: dimension cut (default is d÷2).\nmask::Vector{Int}: dimensions that are affected by the coupling.\n\nKeyword arguments\n\nhidden_dim::Int: number of hidden dimensions in s and t (default is 32).\nn_sublayers_t::Int: number of sublayers in t (default is 2).\nn_sublayers_s::Int: number of sublayers in s (default is 2).\nσ::Function: activation function (default is Flux.relu).\nbias::Bool: activate bias (default is true).).\n\nExample\n\njulia> @summary CouplingBlock(3, [1, 3], n=2, hidden_dim=10, n_sublayers_s=1, σ=Flux.tanh)\nRNVPCouplingLayer | s_net > [3, 10, 2] (62 parameters)\n                  | t_net > [3, 10, 10, 2] (172 parameters)\n                  | axes  > (d,n)=(3,2); identity=(2), transformed=(1,3)\nRNVPCouplingLayer | s_net > [4, 10, 1] (61 parameters)\n                  | t_net > [4, 10, 10, 1] (171 parameters)\n                  | axes  > (d,n)=(3,2); identity=(1,3), transformed=(2)\n\n\n\n\n\n","category":"type"},{"location":"api_coupling/#DensityFlows.RNVPCouplingLayer","page":"Couplings","title":"DensityFlows.RNVPCouplingLayer","text":"RNVPCouplingLayer(s_net, t_net, axes)\n\nStructure of a Real-Non-Volume-Preserving affine coupling layer.\n\nContains \n\n\n\n\n\n","category":"type"},{"location":"api_coupling/#Specific-functions","page":"Couplings","title":"Specific functions","text":"","category":"section"},{"location":"api_coupling/#DensityFlows.RNVP_backward","page":"Couplings","title":"DensityFlows.RNVP_backward","text":"RNVP_backward(s, t, u, axis_id, axis_af)\n\nReturn z = (u-t)*exp(s), ln(det|J|) = sum(s).\n\n\n\n\n\n","category":"function"},{"location":"api_coupling/#DensityFlows.NICE_backward","page":"Couplings","title":"DensityFlows.NICE_backward","text":"NICE_backward(t, u, axis_id, axis_af)\n\nReturn z = (u-t), zeros(...).\n\n\n\n\n\n","category":"function"},{"location":"api_data/#Data","page":"Data","title":"Data","text":"","category":"section"},{"location":"api_data/#Structures","page":"Data","title":"Structures","text":"","category":"section"},{"location":"api_data/#DensityFlows.DataArrays","page":"Data","title":"DensityFlows.DataArrays","text":"DataArrays(x, θ = dflt_θ(x); f_training = 0.9, f_validation = 0.1, rng = Random.default_rng())\n\nNormalised and partitioned data to feed the neural network.\n\nx must be of size (d, ...) where d is the number of physical dimensions.  θ must be of size (n, ...)where n is the number of parameters and every  other array dimensions in place of ... should match that of x. \n\nwarning: Warning\nData is partitioned along the second axis only. It is thus necessary to make sure that size(x, 2)  is large enough by swapping some of the dimensions if necessary.\n\nSee also DataPartition and MetaData.\n\n\n\n\n\n","category":"type"},{"location":"api_data/#DensityFlows.MetaData","page":"Data","title":"DensityFlows.MetaData","text":"MetaData(hash, θ_min, θ_max)\n\nMetadata containing an identification hash value and the boundaries of the parameters array θ.\n\n\n\n\n\n","category":"type"},{"location":"api_data/#DensityFlows.DataPartition","page":"Data","title":"DensityFlows.DataPartition","text":"DataPartition(n, f_training = 0.9, f_validation = 0.1, rng = Random.default_rng())\n\nRandom partition of the data.\n\nThe data is devided into a fraction f_training of training data and  a fraction f_validation of validation data. If f_training + f_validation < 1 the rest is kept as testing data. \n\n\n\n\n\n","category":"type"},{"location":"api_data/#Normalization","page":"Data","title":"Normalization","text":"","category":"section"},{"location":"api_data/#DensityFlows.NormalizationLayer","page":"Data","title":"DensityFlows.NormalizationLayer","text":"NormalizationLayer(x, α, β)\n\nCreate an overall normalization layer for overall dataset.\n\nArguments\n\nx::Union{AbstractArray{T, N}, DataArrays{T, N}}: data\nα::T: minimum value of the output\nβ::T: maximum value of the output\n\ntodo: Batch normalization\nMore sophisticated batch normalization layers are not yet implemented.\n\n\n\n\n\n","category":"type"},{"location":"api_data/#DensityFlows.minimum_θ","page":"Data","title":"DensityFlows.minimum_θ","text":"Minimin value of the input parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_data/#DensityFlows.maximum_θ","page":"Data","title":"DensityFlows.maximum_θ","text":"Maximum value of the input parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_data/#DensityFlows.normalize_input","page":"Data","title":"DensityFlows.normalize_input","text":"normalise_input(x, x_min, x_max)\n\nNormalize input between 0 1. \n\n    y = fracx - x_rm minx_rm max - x_rm min\n\nSee also normalize_input! and resize_output.\n\n\n\n\n\n","category":"function"},{"location":"api_data/#DensityFlows.normalize_input!","page":"Data","title":"DensityFlows.normalize_input!","text":"normalise_input!(x, x_min, x_max)\n\nNormalize input between 0 1 in place. \n\nSee also normalize_input and resize_output!.\n\n\n\n\n\n","category":"function"},{"location":"api_data/#DensityFlows.resize_output","page":"Data","title":"DensityFlows.resize_output","text":"normalise_input(x, x_min, x_max)\n\nResize the output between x_rm min x_rm max. \n\n    x = (x_rm max - x_rm min) y + x_rm min\n\nSee also resize_output! and normalize_input.\n\n\n\n\n\n","category":"function"},{"location":"api_data/#DensityFlows.resize_output!","page":"Data","title":"DensityFlows.resize_output!","text":"normalise_input!(x, x_min, x_max)\n\nResize the output between x_rm min x_rm max in place. \n\nSee also resize_output and normalize_input!.\n\n\n\n\n\n","category":"function"},{"location":"api_data/#Other-functions","page":"Data","title":"Other functions","text":"","category":"section"},{"location":"api_data/#DensityFlows.dflt_θ","page":"Data","title":"DensityFlows.dflt_θ","text":"dflt_θ([T = Float32,] dims::Tuple)\ndflt_θ([T = Float32,] dims...)\ndflt_θ(x::AbstractArray)\n\nDefault value of the parameters, setting the first dimension to size 0.\n\nArguments are the same than zeros or ones. If an array x is passed returns an array of the same dimensions than x with the first dimension set to size 0.\n\nExamples\n\njulia> dflt_θ(2, 3)\n0×2×3 Array{Float32, 3}\n\njulia> dflt_θ(ones(2, 4, 5))\n0×4×5 Array{Float64, 3}\n\n\n\n\n\n","category":"function"},{"location":"api_overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"api_overview/#Structure","page":"Overview","title":"Structure","text":"","category":"section"},{"location":"api_overview/#DensityFlows.FlowElement","page":"Overview","title":"DensityFlows.FlowElement","text":"Building blocks of the flow \n\n\n\n\n\n","category":"type"},{"location":"api_overview/#DensityFlows.summarize","page":"Overview","title":"DensityFlows.summarize","text":"summarize(element)\n\nPrint a summary for the FlowElement element.\n\n\n\n\n\n","category":"function"},{"location":"api_overview/","page":"Overview","title":"Overview","text":"The hierarchy of types can be visualised from the following outputs  ","category":"page"},{"location":"api_overview/","page":"Overview","title":"Overview","text":"using DensityFlows\nCouplingLayer <: FlowElement\nRNVPCouplingLayer <: CouplingLayer","category":"page"},{"location":"api_overview/#Evaluation","page":"Overview","title":"Evaluation","text":"","category":"section"},{"location":"api_overview/#DensityFlows.backward","page":"Overview","title":"DensityFlows.backward","text":"backward(f, x [, θ=dflt_θ(x)])\n\nReturn f^-1(x  theta) and J_f^-1(x  theta) where theta is an array of parameters.\n\nArguments\n\nx::AbstractArray{T}: arguments to pass to the flow element.\nθ::AbstractArray{T}: parameters / conditions (default is dflt_θ).\n\n\n\n\n\n","category":"function"},{"location":"api_overview/#DensityFlows.forward","page":"Overview","title":"DensityFlows.forward","text":"forward(f, z [, θ=dflt_θ(z)])\n\nReturn f(z   theta) and J_f(z  theta) where theta is an array of parameters.\n\nArguments\n\nz::AbstractArray{T}: arguments to pass to the flow element.\nθ::AbstractArray{T}: parameters / conditions (default is dflt_θ).\n\n\n\n\n\n","category":"function"},{"location":"api_overview/#DensityFlows.forward!","page":"Overview","title":"DensityFlows.forward!","text":"forward!(f, z [, θ=dflt_θ(z)])\n\nReplace z by f(z   theta) where theta is an array of parameters.\n\nArguments\n\nz::AbstractArray{T}: arguments to pass to the flow element.\nθ::AbstractArray{T}: parameters / conditions (default is dflt_θ).\n\n\n\n\n\n","category":"function"},{"location":"api_overview/#Save-and-load","page":"Overview","title":"Save and load","text":"","category":"section"},{"location":"api_overview/#DensityFlows.save_element","page":"Overview","title":"DensityFlows.save_element","text":"save_element(directory, element; kws...)\n\nRecursively save the FlowElement element's weights in directory.\n\nSetting erase = true force deletes any existing directory with the same name.\n\n\n\n\n\n","category":"function"},{"location":"api_overview/#DensityFlows.load_element","page":"Overview","title":"DensityFlows.load_element","text":"load_element(directory)\n\nLoad any FlowElement saved in directory.\n\n\n\n\n\n","category":"function"},{"location":"api_overview/#DensityFlows.save_flow","page":"Overview","title":"DensityFlows.save_flow","text":"save_flow(directory, flow, erase = false)\n\nSave flow in the folder directory. Clear it before if erase is true.\n\nSee @clear_and_save_flow and @save_flow \n\n\n\n\n\n","category":"function"},{"location":"api_overview/#Macros","page":"Overview","title":"Macros","text":"","category":"section"},{"location":"api_overview/#DensityFlows.@auto_forward!","page":"Overview","title":"DensityFlows.@auto_forward!","text":"auto_forward!(T)\n\nAutomatically define a forward! function for type T  from forward if there is no possible optimization to  be found in  writting a specific forward! function.\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@auto_functor","page":"Overview","title":"DensityFlows.@auto_functor","text":"auto_functor(T)\n\nAutomatically define a functor function for type T calling forward.\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@summary","page":"Overview","title":"DensityFlows.@summary","text":"auto_functor(element)\n\nCall summarize(element).\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@flow_wrapper","page":"Overview","title":"DensityFlows.@flow_wrapper","text":"flow_wrapper(funcs...)\n\nDefine the flow version of functions funcs defined otherwise for FlowElement with signature (::FlowElement, AbstractArray{T}, AbstractArray{T}), f(flow::Flow, y, θ) = f(flow.model, y, normalizeinput(θ, flow.metadata.θmin, flow.metadata.θ_max))\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@unconditional_wrapper","page":"Overview","title":"DensityFlows.@unconditional_wrapper","text":"unconditional_wrapper(funcs...)\n\nDefine the unconditional version of a function with signature f(::Flow, ::AbstractArray, ::AbstractArray) or f(::FlowElement, ::AbstractArray, ::AbstractArray).\n\nReplace f(obj, y, θ) = f(obj, y, dflt_θ(y))\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@save_as_atomic","page":"Overview","title":"DensityFlows.@save_as_atomic","text":"save_as_atomic(T)\n\nMake structure T saved in a single JLD2 file.\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@clear_and_save_flow","page":"Overview","title":"DensityFlows.@clear_and_save_flow","text":"clear_and_save_flow(directory, flow)\n\nClear any pre-existing folder with name directory and save flow in that folder.\n\nSee save_flow and @save_flow \n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@save_flow","page":"Overview","title":"DensityFlows.@save_flow","text":"save_flow(directory, flow)\n\nSave flow in folder directory.\n\n\n\n\n\n","category":"macro"},{"location":"api_overview/#DensityFlows.@load_flow","page":"Overview","title":"DensityFlows.@load_flow","text":"load_flow(directory, flow)\n\nLoad flow from folder directory.\n\n\n\n\n\n","category":"macro"},{"location":"api_flow/#Flow","page":"Flow","title":"Flow","text":"","category":"section"},{"location":"api_flow/#DensityFlows.Flow","page":"Flow","title":"DensityFlows.Flow","text":"Flow([base, ] model, data)\n\nCreate a Flow from a model FlowChain and for a specific data.\n\nThe data must be passed as a DataArrays.  A spceific base distribution can be given, default is multivariate gaussian. \n\n\n\n\n\n","category":"type"},{"location":"api_flow/#Chains","page":"Flow","title":"Chains","text":"","category":"section"},{"location":"api_flow/#DensityFlows.FlowChain","page":"Flow","title":"DensityFlows.FlowChain","text":"FlowChain(elements::Tuple)\nFlowChain(elements...)\nFlowChain([T = CouplingBlock, ], n, args...; kwars... )\n\nInstanciate a chain of flow elements from a Tuple.\n\nPossible to directly pass the elements of a chain, or construct a chain of n identical blocks of type T, with args...  and kws... passed to the constructor of T.\n\n\n\n\n\n","category":"type"},{"location":"api_flow/#DensityFlows.concatenate","page":"Flow","title":"DensityFlows.concatenate","text":"concatenate(x::FlowChain...)\nconcatenate(x::FlowChain, y::FlowElement...)\nconcatenate(x::Union{Tuple, FlowElement}, y::FlowChain...)\n\nMake one FlowChain from multiple chains or adding flow elements.\n\n\n\n\n\n","category":"function"},{"location":"api_flow/#Density-evaluation","page":"Flow","title":"Density evaluation","text":"","category":"section"},{"location":"api_flow/#StatsBase.sample","page":"Flow","title":"StatsBase.sample","text":"sample([rng=default_rng(), ] flow, dims [, θ = dflt_θ(T, dims)] )\n\nSample the flow distribution.\n\nGive a sample of the flow distribution of size given by dims which can be an Integer, or NTuple of integers. The default random sampler is Random.default_rng() but any sampler of type Random.AbstractRNG can be used. \n\nIf the flow is conditional, θ can be passed as an array  of size (n, dims...) where n is the number of parameters  if θ different for at least two sampled points. Otherwise  θ can be given as a NTuple{n, T}. In that case all points  are drawn with the same θ parameter.\n\nExample\n\n# return a sample of size (20, 10)\nsample(flow, (20, 10))\n\n# return a sample of size (20, 10) at \n# parameter (1f0, 2f0) if flow is conditional\nsample(flow, (20, 10), (1f0, 2f0))\n\n\n\n\n\n","category":"function"},{"location":"api_flow/#Distributions.logpdf","page":"Flow","title":"Distributions.logpdf","text":"logpdf(flow, x [, θ = dflt_θ(x)])\n\nNatural logarithm of the probability density function given by the flow.\n\nArgument x can be given as an array of size (d, dims...) or as a Tuple of d vectors of each length. In the latter case, return the logpdf on a grid of values defined by these vectors.\n\nIf the flow is conditional, θ can be passed as an array  of size (n, dims...) where n is the number of parameters  if θ different for at least two sampled points. Otherwise  θ can be given as a NTuple{n, T}.\n\nwarning: Warning\nIf x is given as a Tuple of vectors,  θ must be passed as a NTuple{n, T}.\n\nExample\n\n# Give arrays in the training range of the flow\nx = range(1f0, 10f0, 40)\ny = range(-2.5f0, 11f0, 10)\nz = range(0.1f0, 2f0, 30)\n\n# For a given trained Flow 'flow' on 3 dimensions\nres = logpdf(flow, (x, y, z)) # if unconditional\nres = logpdf(flow, (x, y, z), (1f0, 2f0)) # if 2 conditions\n\n# using contour in Plots\ncontour(x, y, res[:, :, 1]')\ncontour(x, z, res[:, 4, :]')\n\nSee also pdf.\n\n\n\n\n\n","category":"function"},{"location":"api_flow/#Distributions.pdf","page":"Flow","title":"Distributions.pdf","text":"pdf(flow, x, [, θ = dflt_θ(x)])\n\nProbability density function given by the flow.\n\nSee also logpdf.\n\n\n\n\n\n","category":"function"},{"location":"api_flow/#Training","page":"Flow","title":"Training","text":"","category":"section"},{"location":"api_flow/#DensityFlows.train!","page":"Flow","title":"DensityFlows.train!","text":"train!(flow, data, state; kws...)\n\nTrain the flow to match the input distribution.\n\nArguments\n\nflow::Flow{T}: flow to train\ndata::DataArrays{T}: data to train on\nstate::NamedTuple: optimiser state\n\nKeyword arguments\n\nepochs::Int: number of epochs, default is 100\nbatchsize::Int: size of each batch, default is 64\nshuffle::Bool: if true, shuffle the data in batches, default is true\nverbose::Bool: if true, output the current state, default is true\ndebug::Bool: if true, print extra outputs in case of a strange behaviour, default is false\n\n\n\n\n\n","category":"function"},{"location":"api_flow/#DensityFlows.validation_loss","page":"Flow","title":"DensityFlows.validation_loss","text":"validation loss of all previous states \n\n\n\n\n\n","category":"function"},{"location":"api_flow/#DensityFlows.training_loss","page":"Flow","title":"DensityFlows.training_loss","text":"training loss of all previous steps \n\n\n\n\n\n","category":"function"},{"location":"documentation/#Documentation","page":"Documentation","title":"Documentation","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Let us assume a d-dimensional dataset organised as an Array{Float32} called x such that size(x, 1) = d. Further assume n parameters / conditions gathered into a Array{Float32} called θ such that size(θ, 1) = n. To each x value should correspond one θ value, i.e size(x)[2:end] == size(θ)[2:end]. If these conditions are satisfied one can find the distribution of x knowing theta from the following steps.","category":"page"},{"location":"documentation/#Prepare-the-data","page":"Documentation","title":"Prepare the data","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"For convinivence the data can be stored in a DataArrays object as shown below. For the purpose of this quick start guide we use dummy normal distributed variables. See the example section for a more realistic scenario. ","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"using Flux\nusing Optimisers\nusing LinearAlgebra\nusing Distributions\nusing DensityFlows\n\nx = randn(Float32, 7, 100)\nθ = randn(Float32, 2, 100)\n\ndata = DataArrays(x, θ, f_training = 0.8, f_validation=0.2)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"The sum of the training and validation fractions must be below or equal to 1. They are used to randomly partition the data before training.","category":"page"},{"location":"documentation/#Elementary-layers","page":"Documentation","title":"Elementary layers","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"The elementary elements of the flow are the CouplingLayers. A coupling layer is more particularly a bijective transformation which acts on some dimensions while leaving the others unchanged. In order to perform a transformation of all dimensions a chain of coupling layers is necessary. All definitions of coupling layer below are equivalent and transform dimensions 4, 5, 6, and 7.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"CouplingLayer(data)\nCouplingLayer(data, 3)\nCouplingLayer(data, [4, 5, 6, 7])\nCouplingLayer(7, [4, 5, 6, 7], n=2)\nCouplingLayer(7, 3, n=2)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"and gives","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@summary CouplingLayer(7, 3, n=2)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"One can also use the reverse option to rather transform the dimensions 1, 2, and 3","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"CouplingLayer(data, reverse=true)\nCouplingLayer(data, 3, reverse=true)\nCouplingLayer(data, [1, 2, 3])\nCouplingLayer(7, [1, 2, 3], n=2)\nCouplingLayer(7, 3, n=2, reverse=true)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"in which case","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@summary CouplingLayer(7, 3, n=2, reverse=true)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Layers can then be stacked in a FlowChain to create a flow. An example would be","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"chain = FlowChain(\n    CouplingLayer(data, [4, 5, 6, 7]), \n    CouplingLayer(data, [2, 3, 4, 5]), \n    CouplingLayer(data, [7, 1, 2, 3]), \n    NormalizationLayer(x, -1f0, 1f0)\n    )\n@summary chain","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"where good practice is to place a NormalizationLayer at the end to increase the performances of the network and avoid NaNs.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"warning: Mixing dimensions\nWhen using RNVP-layers, as implemented by default, the chain should at least contain several layers with the transformed dimensions shuffled from one layer to the other, in order for all dimensions to be transformed at lease once.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"tip: One dimensional distribution\nRNVP-layers are diagonal in that they only transform one part of the dimensions while taking the other part in input. Emulating a 1 dimensional distribution is thus not straightforward. Emulating a one dimensional distribution with RNVP-layers is nonetheless possible by artificially promoting the distribution to two dimensions, associating any point to a value drawn from a known distribution like a Gaussian distribution.","category":"page"},{"location":"documentation/#Blocks:-combination-of-layers","page":"Documentation","title":"Blocks: combination of layers","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"A CouplingBlock is a combination of two layers, with complentary transformations on axes. That is, if the first layer transforms dimensions 1, 3, 4, 7 the second transforms dimensions 2, 5, and 6 by construction. Said differently, the following chains are equivalent","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"chain = FlowChain(\n    CouplingLayer(data, [4, 5, 6, 7]), \n    CouplingLayer(data, [1, 2, 3]), \n    )\n@summary chain","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"chain = FlowChain(\n    CouplingBlock(data, [4, 5, 6, 7]), \n    )\n@summary chain","category":"page"},{"location":"documentation/#More-about-the-layers","page":"Documentation","title":"More about the layers","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"By default coupling layers are set as Real-NVP which requires two neural networks called s for scaling and t for translation. Both of these networks are instances of Flux.Dense but their properties can also be modified from the CouplingLayer constructor. First, the following two declarations are equivalent.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@summary CouplingLayer(data)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@summary CouplingLayer(RNVPCouplingLayer, data)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Second, properties of the s and t network can be changed as follows.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@summary CouplingLayer(data, n_sublayers_s=3, n_sublayers_t=4, hidden_dim_s=16, hidden_dim_t=12, σ_s=Flux.relu, σ_t=Flux.sigmoid)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Finally then can also be set direclty but one then needs to be carefull with the input and output dimensions. The input dimension should be number of untransformed dimensions + n and the output dimension should be number of transformed dimensions.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"s_net = Flux.Chain([Flux.Dense(5, 32, Flux.sigmoid), Flux.Dense(32, 16, Flux.relu), Flux.Dense(16, 4)])\nt_net = Flux.Chain([Flux.Dense(5, 12, Flux.relu), Flux.Dense(12, 16, Flux.logcosh), Flux.Dense(16, 32, Flux.relu), Flux.Dense(32, 4)])\n@summary CouplingLayer(s_net, t_net, data)","category":"page"},{"location":"documentation/#Train-the-model","page":"Documentation","title":"Train the model","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"First define the flow from the a chain of layers and a base distribution. By default the latter is set to a multivariate Normal distribution but any distribution from the Distributions package can be used.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"flow = Flow(chain, data)\n\n# which is equivalent to\nbase = Distributions.MvNormal(zeros(Float32, 7), LinearAlgebra.diagm(ones(FLoat32, 7)))\nflow = Flow(base, chain, data)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Then, one needs to define the state of the model and the optimiser.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"state = Optimisers.setup(Optimisers.Adam(1f-3), flow.model)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Finally one can call the implemented train! function.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"train!(flow, data, state, epochs=100, batchsize=64)","category":"page"},{"location":"documentation/#Save-and-load-the-model","page":"Documentation","title":"Save and load the model","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"After training the model can be saved.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"# if directory \"my_flow\" does not already exists\n@save_flow \"my_flow\" flow\n\n# to overwrite any existing directory / model with the same name\n@clear_and_save_flow \"my_flow\" flow ","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"Similarly it can also be loaded back.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"flow = @load_flow \"my_flow\"","category":"page"},{"location":"documentation/#Use-the-model","page":"Documentation","title":"Use the model","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"The model can be used to sample new data points or to extract the probability distribution function. To sample a (d r s), with (r s)in mathbbN_*^2 array of an unconditional flow simply one calls the function sample.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"sample(flow, (r, s))","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"If the flow is conditional, values for those conditions theta need to be provided. There are two possible ways, either you can specify a different value of theta for each drawn value or give a single value for the entire sample. In the first case one must define θ::AbstractArray{T, k} where k=3 (in this example) and of size (n r s). In the second case, one can introduce the condition as a tuple of size n, θ::NTuple{n, T}.  One then calls the same function sample.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"sample(flow, (r, s), θ)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"The model can also be used to directly extract the value of the probability distribution function. For a d-dimensional flow, define NTuple{d, Vector{T}} where every entry is a vector of values where the pdf must be evaluated. For instance, in the case of an unconditional 3-dimensional flow, one can compute the pdf on points (x=2 y=3 z=1), (x=2 y=2 z=1), (x=2 y=3 z=4) from the following call to pdf.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"res = pdf(flow, ([2], [3, 2], [1, 4]))\n\n# res[1, 1, 1] = pdf(2, 3, 1)\n# res[1, 1, 2] = pdf(2, 3, 4)\n# res[1, 2, 1] = pdf(2, 2, 1)\n# res[1, 2, 2] = pdf(2, 2, 4)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"For a conditional flow, as for sampling case, the conditions can be passed as a tuple of size n, θ::NTuple{n, T}.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"res = pdf(flow, ([2], [3, 2], [1, 4], θ)","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"info: logpdf\nThe natural logarithm of the probability distribution funtion ln p can also be obtained similarly calling logpdf.","category":"page"},{"location":"documentation/#To-go-further:-define-custom-layers","page":"Documentation","title":"To go further: define custom layers","text":"","category":"section"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"The code has been built such that it is easy to implement new layers. It must take the form of a struct on which is applied the Flux.@layer macro specifying the trainable parameters. Then it must define a forward and backward functions that are differentiable by Zygote (or use a custom chain rule). These two functions must have signature (::NewLayer, ::AbstractArray{T,N}, ::AbstractArray{T,N}) where {T,N} and return the transformed array as well as the logarithm of the determinant of the jacobian of the transformation.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"struct NewLayer{U, V} <: FlowElement\n    a::U\n    b::V\nend\n\n# make the coupling layer parameters trainable\nFlux.@layer NewLayer trainable=(a,)\n\n# dummy forward and backward functions\nforward(layer::NewLayer, z::AbstractArray{T,N}, θ::AbstractArray{T,N}) where {T,N} = a .* z .+ b, a\nbackward(layer::NewLayer, x::AbstractArray{T,N}, θ::AbstractArray{T,N}) where {T,N} = (x .- b) ./ a, one(T)/a","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"One must then also define a forward! function. One can define a custom one or use a macro to define a default. Similarly using another macro one can also define a functor in place of forward(...) to call layer(...) but this is optional.","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@auto_forward! NewLayer\n@auto_functor NewLayer","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"By default such a structure is saved as two files, one for a and one for b if both are simple elements everything can be saved in a single file using the macro","category":"page"},{"location":"documentation/","page":"Documentation","title":"Documentation","text":"@save_as_atomic NewLayer","category":"page"},{"location":"example/#Example","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Let us assume that we have the following data","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"n = 2000\n\n# parameter that is fixed to -1 or 2\nθ = Matrix{Float32}(undef, (1, 2*n))\n@views θ[1, 1:n] .= -1f0\n@views θ[1, (n+1):end] .= 2f0\n\nx1 = randn(2*n)\nx2 = sin.(x1/0.8) .+ 0.3*randn(2*n) .+ θ[1, :]\nx3 = exp.(x1/1.4)/10 .+  0.1*θ[1, :] .*randn(2*n) .- 0.1 * θ[1, :]\nx4 = cos.(x1/1.1) .+ 0.3*randn(2*n) .+ θ[1, :]\nx5 = randn(2*n)\n\nx = Float32.(vcat(x1', x2', x3', x4', x5'))","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"These data points are represented in the figure below as marks. The underlying true distribution is represented as shaded areas and the corresponding 1D marginalisation is shown on the diagonal.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: )","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Let us now use DensityFlows to emulate the probability distribution frunction from the sample. The first step is to define the DataArrays object from both x and theta and the FlowChain. In the FlowChain one can either put CouplingBlocks or series of CouplingLayers making sure that we suffle the transformed dimensions as we do here.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using DensityFlows\n\ndata = DataArrays(x, θ)\n\nchain = FlowChain(\n    CouplingLayer(data, [1, 2, 3], hidden_dim_s=16,  hidden_dim_t=16), \n    CouplingLayer(data, [3, 4, 5], hidden_dim_s=16,  hidden_dim_t=16), \n    CouplingLayer(data, [5, 1, 2], hidden_dim_s=16,  hidden_dim_t=16), \n    NormalizationLayer(x, -1f0, 1f0)\n    )\n\n@summary flow = Flow(chain, data)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Training is performed with the train! function, after setting the optimizer.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"state = Optimisers.setup(Optimisers.Adam(1f-4), flow.model)\ntrain!(flow, data, state, epochs=50)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The train and validation loss can be obtained as shown below.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"p = plot()\nplot!(p, training_loss(flow))\nplot!(p, validation_loss(flow))","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: )","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we can compare the reconstructed distribution (dark contours) to the true distribution (light contours). We can see that in that case a sample of 2000 points is enough to have a good emulator of the true distribution. ","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: )","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"New points can be sampled using","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"x_new_1 = sample(flow, 1000, (-1f0,))\nx_new_2 = sample(flow, 1000, (2f0,))","category":"page"}]
}
